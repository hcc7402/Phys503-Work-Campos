{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hcc7402/Phys503-Work-Campos/blob/week12/homework_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDKqVDhPWs9W"
      },
      "source": [
        "# Homework 11: Detecting Distribution Shift on MNIST using Bayesian Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhJ5XZK9Ws9Z"
      },
      "source": [
        "## <span style=\"color:Orange\">Overview</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-bJ1LbSpXy2"
      },
      "source": [
        "In this exercise we will compare Bayesian NNs with deterministic NNs on a distribution shift detection task. To do this, we'll monitor the predictive entropy as the distribution gradually shifts.  A model with better uncertainty quantification should become less certain---that is, have a more entropic predictive distribution---as the input distribution shifts.  Mathematically, our quantity of interest is:\n",
        "\n",
        "$$ \\Large\n",
        "\\mathbb{H}[y | x^{*}, D] = - \\sum_{y} p(y | x^{*}, D) \\log p(y | x^{*}, D)\n",
        "$$\n",
        "\n",
        "where $p(y | x^{*}, D)$ is the predictive distribution:\n",
        "\n",
        "$$ \\Large\n",
        "p(y | x^{*}, D) = \\int_{\\theta} p(y | x^{*}, \\theta) \\ p(\\theta | D) \\ d \\theta.\n",
        "$$\n",
        "\n",
        "The goal is to obtain something similar to Figure #4 from the paper [Multiplicative Normalizing Flows for Variational Bayesian Neural Networks](https://arxiv.org/abs/1603.04733), comparing MC dropout, ensembles, and a Bayesian NN.\n",
        "\n",
        "We will be using the well-known MNIST dataset, a set of 70,000 hand-written digit images, and we will generate a gradual distribution shift on the dataset by rotating the images. As such, the final plot will depict the change in the entropy of the predictive distribution (y-axis) as degree of rotation increases (x-axis). The paper above shows the result for one image.  We, on the other hand, will average over multiple images to make a better comparison between models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y2x-rBnWs9a"
      },
      "source": [
        "We'll use rotation to simulate a smooth shift. Here's how you can rotate a given image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M0iR5klWs9a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "from torch.nn.functional import softmax\n",
        "from torchvision.transforms.functional import rotate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJK1yniMWs9b"
      },
      "outputs": [],
      "source": [
        "def imshow(image):\n",
        "    plt.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yc0WLkNWs9b"
      },
      "outputs": [],
      "source": [
        "def show_rotation_on_mnist_example_image():\n",
        "    mnist_train = datasets.MNIST('./tmp_data', train=True, download=True)\n",
        "    image = Image.fromarray(mnist_train.data[0].numpy())\n",
        "    imshow(image)\n",
        "    rotated_image = rotate(image, angle=90)\n",
        "    imshow(rotated_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdQSHXvzWs9c"
      },
      "outputs": [],
      "source": [
        "show_rotation_on_mnist_example_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj3LI711Ws9c"
      },
      "source": [
        "Let's setup the training and testing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si5_hY3aWs9c"
      },
      "outputs": [],
      "source": [
        "def get_mnist_data(train=True):\n",
        "    mnist_data = datasets.MNIST('../data', train=train, download=True)\n",
        "    x = mnist_data.data.reshape(-1, 28 * 28).float()\n",
        "    y = mnist_data.targets\n",
        "    return x, y\n",
        "\n",
        "x_train, y_train = get_mnist_data(train=True)\n",
        "x_test, y_test = get_mnist_data(train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPQHF2VhWs9c"
      },
      "source": [
        "Now that we have the data, let's start training neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYe80AYVWs9d"
      },
      "source": [
        "## <span style=\"color:Orange\">Define non-Bayesian (Deterministic) Neural Network</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoR8G5X5Ws9d"
      },
      "source": [
        "We will reuse our MLP network architecture with different hyperparameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwqh78ZjWs9d"
      },
      "source": [
        "First let's create our point estimate neural network, in other words a standard fully connected MLP. We will define the number of hidden layers dynamically so we can reuse the same class for different depths.  We will also add a ___<span style=\"color:violet\">dropout</span>___ flag, this will allow us to easily use the same architecture for our BNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyYUpD1cWs9d"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=1, output_dim=1, hidden_dim=10, n_hidden_layers=1, use_dropout=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_dropout = use_dropout\n",
        "        if use_dropout:\n",
        "            self.dropout = nn.Dropout(p=0.5)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "        # dynamically define architecture\n",
        "        self.layer_sizes = [input_dim] + n_hidden_layers * [hidden_dim] + [output_dim]\n",
        "        layer_list = [nn.Linear(self.layer_sizes[idx - 1], self.layer_sizes[idx]) for idx in\n",
        "                      range(1, len(self.layer_sizes))]\n",
        "        self.layers = nn.ModuleList(layer_list)\n",
        "\n",
        "    def forward(self, input):\n",
        "        hidden = self.activation(self.layers[0](input))\n",
        "        for layer in self.layers[1:-1]:\n",
        "            hidden_temp = self.activation(layer(hidden))\n",
        "\n",
        "            if self.use_dropout:\n",
        "                hidden_temp = self.dropout(hidden_temp)\n",
        "\n",
        "            hidden = hidden_temp + hidden  # residual connection\n",
        "\n",
        "        output_mean = self.layers[-1](hidden).squeeze()\n",
        "        return output_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tQQPQjWs9d"
      },
      "source": [
        "## <span style=\"color:Orange\">Problem 1: Deterministic Neural Network</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY-2rK8WWs9d"
      },
      "outputs": [],
      "source": [
        "net = MLP(input_dim=784, output_dim=10, hidden_dim=30, n_hidden_layers=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Xq-mYUWs9d"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Training</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXqMB9ONWs9d"
      },
      "outputs": [],
      "source": [
        "def train_on_mnist(net):\n",
        "    x_train, y_train = get_mnist_data(train=True)\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    batch_size = 250\n",
        "\n",
        "    progress_bar = trange(20)\n",
        "    for _ in progress_bar:\n",
        "        for batch_idx in range(int(x_train.shape[0] / batch_size)):\n",
        "            batch_low, batch_high = batch_idx * batch_size, (batch_idx + 1) * batch_size\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(target=y_train[batch_low:batch_high], input=net(x_train[batch_low:batch_high]))\n",
        "            progress_bar.set_postfix(loss=f'{loss / batch_size:.3f}')\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV7MctdqWs9e"
      },
      "outputs": [],
      "source": [
        "net = train_on_mnist(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSXgeynuWs9e"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Testing</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_NaGFV-Ws9e"
      },
      "outputs": [],
      "source": [
        "def accuracy(targets, predictions):\n",
        "  return (targets == predictions).sum() / targets.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3-TyEK2Ws9e"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy_on_mnist(net):\n",
        "    test_data = get_mnist_data(train=False)\n",
        "    x_test, y_test = test_data\n",
        "    net.eval()\n",
        "    y_preds = net(x_test).argmax(1)\n",
        "    acc = accuracy(y_test, y_preds)\n",
        "    print(\"Test accuracy is %.2f%%\" % (acc.item() * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzyRW2p_Ws9e"
      },
      "outputs": [],
      "source": [
        "evaluate_accuracy_on_mnist(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E54pABb4Ws9e"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Rotating the Images</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiLMJjbsWs9e"
      },
      "source": [
        "Now let's compute predictive entropy on some rotated images..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Z9dquFWs9e"
      },
      "source": [
        "First we will generate the rotated images with an increasing rotation angle from the test images. We use a subset of the MNIST test set for evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ug2ZKwgWs9e"
      },
      "outputs": [],
      "source": [
        "def get_mnist_test_subset(n_test_images):\n",
        "    mnist_test = datasets.MNIST('../data', train=False, download=True)\n",
        "    x = mnist_test.data[:n_test_images].float()\n",
        "    y = mnist_test.targets[:n_test_images]\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv74OtWUWs9f"
      },
      "outputs": [],
      "source": [
        "n_test_images = 100\n",
        "x_test_subset, y_test_subset = get_mnist_test_subset(n_test_images=n_test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5HLy5eIWs9f"
      },
      "outputs": [],
      "source": [
        "rotation_angles = [3 * i for i in range(0, 31)] # use angles from 0 to 90 degrees\n",
        "rotated_images = [rotate(x_test_subset, angle).reshape(-1, 28 * 28) for angle in rotation_angles]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbe9PWHvWs9f"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Evaluating on the Rotated Images</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA4dZWQ0Ws9g"
      },
      "outputs": [],
      "source": [
        "y_preds_deterministic = [softmax(net(images), dim=-1) for images in rotated_images]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j_6FLNZWs9g"
      },
      "source": [
        "The [information entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) $H$ of a probability distribution $p$ over a discrete random variable $X$ with possible outcomes $x_1, \\ldots, x_N$, occuring with probabilities $p(x_i) := p_i$ is given by:\n",
        "\n",
        "$$ \\Large\n",
        "H(p) = - \\sum_{i=1}^{N} p_i \\log p_i\n",
        "$$\n",
        "\n",
        "The entropy quantifies the uncertainty of a probability distribution in the sense, that the more uncertain the outcome a hypothetical experiment with drawing from the distribution is the higher the entropy. Highest is for an equal distribution of probability mass over all possible outcomes.\n",
        "In our case the deterministic NN estimates a probability distribution over the ten digits as classes on MNIST for each image. For the rotated images we can thus calculate the entropy over the rotation angle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ry0AyV7Ws9h"
      },
      "source": [
        "<span style=\"color:Yellow\">Question:</span> How do you expect the entropy to behave with increasing rotation angle of the images? Answer in the cell below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfamNXQVWs9h"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94wmKtsqWs9h"
      },
      "source": [
        "Implement a function for calculating the entropy according to the formula above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPuKM-RCWs9h"
      },
      "outputs": [],
      "source": [
        "def entropy(p):\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQMQ6iV4Ws9h"
      },
      "source": [
        "Now we can calculate the accuracies and entropies for all rotated images and plot both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-UWGr7ZWs9h"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracies_and_entropies(y_preds):\n",
        "    accuracies = [accuracy(y_test_subset, p.argmax(axis=1)) for p in y_preds]\n",
        "    entropies = [np.mean(entropy(p.detach().numpy())) for p in y_preds]\n",
        "    return accuracies, entropies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxzTyMf0Ws9l"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy_and_entropy(add_to_plot):\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    plt.xlim([0, 90])\n",
        "    plt.xlabel(\"Rotation Angle\", fontsize=20)\n",
        "\n",
        "    add_to_plot(ax)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA3EqpuhWs9l"
      },
      "outputs": [],
      "source": [
        "def add_deterministic(ax):\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_deterministic)\n",
        "    ax.plot(rotation_angles, accuracies, 'b--', linewidth=3, label=\"Accuracy, Deterministic\")\n",
        "    ax.plot(rotation_angles, entropies, 'b-', linewidth=3, label=\"Entropy, Deterministic\")\n",
        "plot_accuracy_and_entropy(add_deterministic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRC1s2RSWs9l"
      },
      "source": [
        "<span style=\"color:Yellow\">Question:</span> What is your interpretation of the plot above: Is the predictive entropy changing? If so, how would you explain this? Answer in the cell below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vl4iet6Ws9m"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5OFhC1Ws9m"
      },
      "source": [
        "## <span style=\"color:Orange\">Problem 2: Monte Carlo Dropout Network</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfSac-ItWs9m"
      },
      "source": [
        "Let's create our Dropout Network. We keep the network depth and hidden layer size the same as for the MLP for a fair model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWOGpU5LWs9m"
      },
      "outputs": [],
      "source": [
        "net_dropout = MLP(input_dim=784, output_dim=10, hidden_dim=30, n_hidden_layers=3, use_dropout=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nBo7PRPWs9m"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Training</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv09jRtwWs9m"
      },
      "outputs": [],
      "source": [
        "net_dropout = train_on_mnist(net_dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz36ZVdfWs9m"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Testing</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V03LHThsWs9m"
      },
      "outputs": [],
      "source": [
        "evaluate_accuracy_on_mnist(net_dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCmE3zZxWs9m"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Evaluating on the Rotated Images</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udlKGkQ_Ws9n"
      },
      "source": [
        "Sample 100 different dropout masks and average the predictions over them (store the predictions in a list called `y_pred_dropout`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7v10HdUWs9n"
      },
      "outputs": [],
      "source": [
        "n_dropout_samples = 100\n",
        "net_dropout.train()  # we set the model to train to 'activate' the dropout layer\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# y_preds_dropout = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx3vCWYkWs9n"
      },
      "source": [
        "<span style=\"color:Yellow\">Question:</span> What is the best way to average over the predictions? Should you first average the network output and then apply the softmax, or the other way around? Answer in the cell below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7U4iYBeWs9n"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-InWise8Ws9n"
      },
      "outputs": [],
      "source": [
        "def add_deterministic_and_dropout(ax):\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_deterministic)\n",
        "    ax.plot(rotation_angles, accuracies, 'b--', linewidth=3, label=\"Accuracy, Deterministic\")\n",
        "    ax.plot(rotation_angles, entropies, 'b-', linewidth=3, label=\"Entropy, Deterministic\")\n",
        "\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_dropout)\n",
        "    ax.plot(rotation_angles, accuracies, 'r--', linewidth=3, label=\"Accuracy, MC Dropout\")\n",
        "    ax.plot(rotation_angles, entropies, 'r-', linewidth=3, label=\"Entropy, MC Dropout\")\n",
        "plot_accuracy_and_entropy(add_deterministic_and_dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PehkWhyMWs9n"
      },
      "source": [
        "<span style=\"color:Yellow\">Question:</span> How does MLP compare with MC-Dropout Network? (Are there any benefits of the Bayesian approach?). Answer in the cell below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRBdRPxwWs9n"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djW8LJZEWs9n"
      },
      "source": [
        "## <span style=\"color:Orange\">Problem 3: Deep Ensemble</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMoU4HgQ1Tmz"
      },
      "source": [
        "Deep ensembles were first introduced by [Lakshminarayanan et al. (2017)](https://arxiv.org/abs/1612.01474). As the name implies multiple point estimate NN are trained, ___<span style=\"color:violet\">an ensemble</span>___, and the final prediction is computed as an average across the models. From a Bayesian perspective the different point estimates correspond to modes of a Bayesian posterior. This can be interpreted as approximating the posterior with a distribution parametrized as multiple Dirac deltas:\n",
        "\n",
        "$$ \\Large\n",
        "q_{\\phi}(\\theta | D) = \\sum_{\\theta_{i} ∈ ϕ} \\alpha_{\\theta_{i}} δ_{\\theta_{i}}(\\theta)\n",
        "$$\n",
        "\n",
        "where $\\alpha_{\\theta_{i}}$ are positive constants such that their sum is equal to one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CIzyImNWs9o"
      },
      "source": [
        "Now let's investigate Deep Ensemble performance. We will use the exact same network hyperparameters as for the MLP:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTz2X14kWs9o"
      },
      "source": [
        "Define and train an ensemble of five MLPs with the same hyperparameters as above. First generate the ensembles (store in a list called `ensemble`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laGW1qdCWs9o"
      },
      "outputs": [],
      "source": [
        "ensemble_size = 5\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# ensemble = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezkaRx-0Ws9o"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Training</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0imtENqWs9o"
      },
      "outputs": [],
      "source": [
        "for net in ensemble:\n",
        "  train_on_mnist(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpywEQGsWs9o"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Testing</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtiH9qmWWs9o"
      },
      "source": [
        "Evaluate the accuracy of the ensemble prediction. ___Hints___: How do you aggregate best over the multiple different predictions given by the members of the ensemble? What is the difference to the regression setting above?v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giGV914YWs9o"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# y_preds = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwGVvXNGWs9p"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Evaluating on the Rotated Images</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTkJPF9RWs9p"
      },
      "source": [
        "Again, average the predictions, but this time over the members of the ensemble (store the predictions in a list called `y_preds_ensemble`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfSQJCs8Ws9p"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# y_preds_ensemble = NotImplemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9EgNLXWWs9p"
      },
      "outputs": [],
      "source": [
        "def add_deep_ensemble(ax):\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_deterministic)\n",
        "    ax.plot(rotation_angles, accuracies, 'b--', linewidth=3, label=\"Accuracy, Deterministic\")\n",
        "    ax.plot(rotation_angles, entropies, 'b-', linewidth=3, label=\"Entropy, Deterministic\")\n",
        "\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_dropout)\n",
        "    ax.plot(rotation_angles, accuracies, 'r--', linewidth=3, label=\"Accuracy, MC Dropout\")\n",
        "    ax.plot(rotation_angles, entropies, 'r-', linewidth=3, label=\"Entropy, MC Dropout\")\n",
        "\n",
        "    accuracies, entropies = calculate_accuracies_and_entropies(y_preds_ensemble)\n",
        "    ax.plot(rotation_angles, accuracies, 'g--', linewidth=3, label=\"Accuracy, Deep Ensemble\")\n",
        "    ax.plot(rotation_angles, entropies, 'g-', linewidth=3, label=\"Entropy, Deep Ensemble\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buf5D4lpWs9p"
      },
      "outputs": [],
      "source": [
        "plot_accuracy_and_entropy(add_deep_ensemble)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwua0TG8Ws9p"
      },
      "source": [
        "<span style=\"color:Yellow\">Question:</span> Are there any differences in the performance? Explain why you see or don't see any differences. Answer in the cell below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nltay1G9Ws9p"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B4689oEWs9p"
      },
      "source": [
        "## [EXTRA CREDIT] <span style=\"color:Orange\">Problem 4: Bayesian Neural Network</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDHZ_ISMWs9p"
      },
      "source": [
        "First install pyro package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OylZs8YWs9q"
      },
      "outputs": [],
      "source": [
        "%pip install pyro-ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTcoSWd8Ws9q"
      },
      "outputs": [],
      "source": [
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.nn import PyroModule, PyroSample\n",
        "from pyro.infer import Predictive\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.infer.autoguide import AutoDiagonalNormal\n",
        "from pyro.distributions import Normal, Categorical\n",
        "from torch.nn.functional import softmax\n",
        "from tqdm.auto import trange, tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1wYH_qGWs9q"
      },
      "source": [
        "Implement a Bayesian Neural Network for classifying MNIST digits. For background on deep Bayesian networks, see the lecture notebook on [Uncertainty Quantification](AIExplainabilityUncertaintyQuantification.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdNQHzlAWs9q"
      },
      "source": [
        "As a backbone use the MLP architecture introduced in the beginning of the notebook. However, because we will implement a custom *guide()*, define every layer explicitly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzeBKW65Ws9q"
      },
      "outputs": [],
      "source": [
        "class My_MLP(nn.Module):\n",
        "  '''\n",
        "  Implement a MLP with 3 hidden layers, Tanh activation, no dropout or residual connections\n",
        "  '''\n",
        "  def __init__(self, in_dim=784, out_dim=10, hid_dim=200):\n",
        "    super().__init__()\n",
        "    assert in_dim > 0\n",
        "    assert out_dim > 0\n",
        "    assert hid_dim > 0\n",
        "\n",
        "    # Define the activation\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    # self.act =\n",
        "\n",
        "    # Define the 3 hidden layers:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    # self.fc1 =\n",
        "    # self.fc2 =\n",
        "    # self.fc3 =\n",
        "    # self.out =\n",
        "\n",
        "  def forward(self, x):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJZWom2KWs9q"
      },
      "source": [
        "Initialize the network. You will have to access it's layers in your model and guide functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5bxt7QWWs9q"
      },
      "outputs": [],
      "source": [
        "net = My_MLP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X1_liMqWs9q"
      },
      "outputs": [],
      "source": [
        "# confirm your layer names\n",
        "for name, _ in net.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16E9X5o3Ws9r"
      },
      "source": [
        "Define the model:\n",
        "> Probablistic models in Pyro are specified as *model()* functions. This function defines how the output data is generated. Within the model() function, first, the pyro module *random_module()* converts the paramaters of our NN into random variables that have prior probability distributions. Second, in pyro *sample* we define that the output of the network is categorical, while the pyro *plate* allows us to vectorize this function for computational efficiency.\n",
        "\n",
        "> Hint: remember we are doing a classification instead of regression!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiF1I9a6Ws9r"
      },
      "source": [
        "You can 'cheat' a little: to speed up the training and limit a bit more the number of paramters we need to optimize, implement a BNN where only the **last layer** is Bayesian!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnOY0EFTWs9r"
      },
      "outputs": [],
      "source": [
        "def model(x_data, y_data):\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW5_SJAtWs9r"
      },
      "source": [
        "implement the guide(), *variational distribution*:\n",
        "> the guide allows us to initialise a well behaved distribution which later we can optmize to approximate the true posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgYQTzwtWs9r"
      },
      "outputs": [],
      "source": [
        "softplus = torch.nn.Softplus()\n",
        "\n",
        "def my_guide(x_data, y_data):\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFgEf-IgWs9r"
      },
      "source": [
        "Initialize the stochastic variational inference (SVI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2c5UaKmWs9r"
      },
      "outputs": [],
      "source": [
        "adam = pyro.optim.Adam({\"lr\": 1e-3})\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# svi = raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIjbhm9DWs9r"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Training</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PxF7iSIWs9r"
      },
      "outputs": [],
      "source": [
        "pyro.clear_param_store()\n",
        "batch_size = 250\n",
        "bar = trange(30)\n",
        "for epoch in bar:\n",
        "  for batch_idx in range(int(x_train.shape[0] / batch_size)):\n",
        "    batch_low, batch_high = batch_idx * batch_size, (batch_idx+1) * batch_size\n",
        "    loss = svi.step(x_train[batch_low:batch_high], y_train[batch_low:batch_high])\n",
        "    bar.set_postfix(loss=f'{loss / batch_size:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfTpl_s4Ws9r"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Testing</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leYVggcQWs9s"
      },
      "source": [
        "Use the learned *guide()* function to do predictions. Why? Because the *model()* function knows the **priors** for the weights and biases, **not** the learned posterior. The *guide()* contains the approximate posterior distributions of the parameter values, which we want to use to make the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnzZAHz9Ws9s"
      },
      "outputs": [],
      "source": [
        "num_samples = 10\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# y_preds = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljnqpuLsWs9s"
      },
      "source": [
        "### <span style=\"color:LightGreen\">Evaluating on Rotated Images</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm7lSuX1Ws9s"
      },
      "source": [
        "Store the predictions in a list called `y_preds_bnn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUDs4yptWs9s"
      },
      "outputs": [],
      "source": [
        "num_samples = 50\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "# y_preds_bnn = NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNydJ7qxWs9s"
      },
      "source": [
        "Show entropies for all four models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ2mJuEJWs9s"
      },
      "outputs": [],
      "source": [
        "# add the computed values for BNN\n",
        "\n",
        "def add_bnn(ax):\n",
        "\n",
        "  accuracies, entropies = calculate_accuracies_and_entropies(y_preds_deterministic)\n",
        "  ax.plot(rotation_angles, accuracies, 'b--', linewidth=3, label=\"Accuracy, Deterministic\")\n",
        "  ax.plot(rotation_angles, entropies, 'b-', linewidth=3, label=\"Entropy, Deterministic\")\n",
        "\n",
        "  accuracies, entropies = calculate_accuracies_and_entropies(y_preds_dropout)\n",
        "  ax.plot(rotation_angles, accuracies, 'r--', linewidth=3, label=\"Accuracy, MC Dropout\")\n",
        "  ax.plot(rotation_angles, entropies, 'r-', linewidth=3, label=\"Entropy, MC Dropout\")\n",
        "\n",
        "  accuracies, entropies = calculate_accuracies_and_entropies(y_preds_ensemble)\n",
        "  ax.plot(rotation_angles, accuracies, 'g--', linewidth=3, label=\"Accuracy, Deep Ensemble\")\n",
        "  ax.plot(rotation_angles, entropies, 'g-', linewidth=3, label=\"Entropy, Deep Ensemble\")\n",
        "\n",
        "  accuracies, entropies = calculate_accuracies_and_entropies(y_preds_bnn)\n",
        "  ax.plot(rotation_angles, accuracies, 'y--', linewidth=3, label=\"Accuracy, BNN\")\n",
        "  ax.plot(rotation_angles, entropies, 'y-', linewidth=3, label=\"Entropy, BNN\")\n",
        "\n",
        "plot_accuracy_and_entropy(add_bnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oPlfjEWWs9s"
      },
      "source": [
        "<span style=\"color:Yellow\">Question:</span> Which method is the best at detecting the distribution shift? How can you interpret this? Answer in the cell below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLmIWBO7Ws9s"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6txtSNzDWs9s"
      },
      "source": [
        "## <span style=\"color:Orange\">Acknowledgments</span>\n",
        "\n",
        "* Initial version: Mark Neubauer\n",
        "\n",
        "© Copyright 2025"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}